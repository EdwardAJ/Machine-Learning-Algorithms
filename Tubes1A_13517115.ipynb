{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Tugas Besar Bagian A </h1>\n",
    "\n",
    "Oleh:\n",
    "- Ignatius Timothy Manullang / 13517044\n",
    "- Fatur Rahman / 13517056\n",
    "- Fata Nugraha / 13517109\n",
    "- Edward Alexander Jaya / 13517115\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Pembacaan Dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import all dependencies\n",
    "\n",
    "from id3 import Id3Estimator\n",
    "from id3 import export_text\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pembelajaran dataset Iris </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Model for iris:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of labels=3 does not match number of samples=150",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4f639bae7ad0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdecision_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdecision_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mDecisionTreeModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miris\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecisionTreeModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[0;32m--> 265\u001b[0;31m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels=3 does not match number of samples=150"
     ]
    }
   ],
   "source": [
    "decision_tree = tree.DecisionTreeClassifier()\n",
    "decision_tree = decision_tree.fit(iris.data, iris.target_names)\n",
    "DecisionTreeModel = tree.export_text(decision_tree, feature_names=iris['feature_names'])\n",
    "print(DecisionTreeModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ID3 Model for iris:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Id3Estimator()\n",
    "estimator = estimator.fit(iris.data, iris.target)\n",
    "\n",
    "ID3Model = export_text(estimator.tree_, feature_names=iris['feature_names'])\n",
    "print(ID3Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pembelajaran dataset play-tennis </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read play-tennis dataset\n",
    "df = pd.read_csv('datasets/play_tennis.csv')\n",
    "\n",
    "# Then drop df['day']\n",
    "df = df.drop('day', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target variable, which is the variable of classes\n",
    "target = df['play']\n",
    "\n",
    "print(\"Target values: \")\n",
    "print(target)\n",
    "\n",
    "# Drop play attribute\n",
    "df = df.drop('play', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to store encoded values\n",
    "df_encoded = df\n",
    "\n",
    "# Use label encoder to encode data\n",
    "LE = LabelEncoder()\n",
    "\n",
    "# Feature names for tree generation purposes\n",
    "feature_names_var = [\"outlook\", \"temp\", \"humidity\", \"wind\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store as a map: key -> encoded value, value -> real value\n",
    "dictOfValues = {}\n",
    "\n",
    "for key in feature_names_var:\n",
    "    # Encode the data\n",
    "    df_encoded[key] = LE.fit_transform(df[key])\n",
    "    \n",
    "    # Map encoded values with real values:\n",
    "    dictOfValues[key] = {}\n",
    "    \n",
    "    for index in range(len(df_encoded[key])):\n",
    "        encoded_value = df_encoded[key][index]\n",
    "        real_value = LE.inverse_transform(df[key])[index]\n",
    "        dictOfValues[key][encoded_value] = real_value\n",
    "\n",
    "for key in feature_names_var:\n",
    "    # Sort\n",
    "    dictOfValues[key] = sorted(dictOfValues[key].items(), key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the encoded data\n",
    "transposed_df_encoded = df_encoded.transpose()\n",
    "\n",
    "# Define data variable\n",
    "data = []\n",
    "for index in range (0, 14):\n",
    "    data.append(transposed_df_encoded[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hasil pembelajaran: </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Model for play-tennis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = tree.DecisionTreeClassifier()\n",
    "decision_tree = decision_tree.fit(data, target)\n",
    "DecisionTreeModel = tree.export_text(decision_tree, feature_names=feature_names_var)\n",
    "\n",
    "print(\"Tree:\")\n",
    "print(DecisionTreeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classFreqRatio = target.value_counts(normalize=True)\n",
    "\n",
    "print(len(classFreqRatio))\n",
    "print(classFreqRatio.keys()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outlook:**\n",
    "- outlook <= 0.50, artinya outlook = Overcast\n",
    "- outlook <= 1.50, artinya outlook = Rain\n",
    "- selain itu, artinya outlook = Sunny\n",
    "\n",
    "<br> **Temp:**\n",
    "- temp <= 0.50, artinya temp = Cool\n",
    "- temp <= 1.50 atau <= 1.00, artinya temp = Hot\n",
    "- selain itu, artinya temp = Mild\n",
    "\n",
    "<br> **Wind**\n",
    "- wind <= 0.50, artinya wind = Weak\n",
    "- selain itu, artinya wind = Strong\n",
    "\n",
    "<br> **Humidity**\n",
    "- humidity <= 0.50, artinya humidity = High\n",
    "- selain itu, artinya humidity = High"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ID3 Model for play-tennis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Id3Estimator()\n",
    "fitEstimator = estimator.fit(data, target)\n",
    "\n",
    "ID3Model = export_text(fitEstimator.tree_, feature_names=feature_names_var)\n",
    "\n",
    "print(\"Tree:\")\n",
    "print(ID3Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outlook:**\n",
    "- outlook <= 0.50, artinya outlook = Overcast\n",
    "- outlook <= 1.50, artinya outlook = Rain\n",
    "- selain itu, artinya outlook = Sunny\n",
    "\n",
    "<br> **Temp:**\n",
    "- temp <= 0.50, artinya temp = Cool\n",
    "- temp <= 1.50 atau <= 1.00, artinya temp = Hot\n",
    "- selain itu, artinya temp = Mild\n",
    "\n",
    "<br> **Wind**\n",
    "- wind <= 0.50, artinya wind = Weak\n",
    "- selain itu, artinya wind = Strong\n",
    "\n",
    "<br> **Humidity**\n",
    "- humidity <= 0.50, artinya humidity = High\n",
    "- selain itu, artinya humidity = High"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Berikut adalah arti setiap kode angka pada setiap atribut:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(sorted(dictOfValues.items(), key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Perbandingan algoritma pada hal 56 buku Machine Learning Tom Mitchell dengan kedua library DecisionTreeClassifier dan Id3Estimator</h3>\n",
    "<h4>2a. Penentuan atribut terbaik</h4>\n",
    "<h5>Algoritma ID3 pada buku Machine Learning Tom Mitchell vs library DecisionTreeClassifier</h5>\n",
    "Berbeda dengan algoritma ID3 pada buku Machine Learning Tom Mitchell, library DecisionTreeClassifier menggunakan Gini gain untuk menentukan atribut terbaik. Gini Gain merupakan penurunan Gini Impurity setelah kumpulan data dibagi pada suatu atribut. Gini Impurity merupakan kemungkinan klasifikasi salah dari elemen yang dipilih secara acak jika dilabel secara acak menurut distribusi kelas di suatu dataset. Gini gain didapatkan dengan mengkalkulasi Gini Impurity dari seluruh dataset, dikurangi dengan Gini Impurity dari setiap branch setelah dilakukan splitting. Atribut terbaik adalah atribut yang memiliki Gini gain tertinggi.\n",
    "<img src=\"gini_equation.png\" />\n",
    "Keterangan:\n",
    "<br> G = Gini Impurity \n",
    "<br> C = jumlah pembagi data / jumlah kelas data\n",
    "<br> p(i) = kemungkinan secara acak memilih elemen dari kelas i\n",
    "<h5>Algoritma ID3 pada buku Machine Learning Tom Mitchell vs library Id3Estimator</h5>\n",
    "Kedua algoritma menentukan atribut terbaik dengan information gain, yang didapat dengan menghitung decrease dari entropy dan mencari atribut yang mengurangi entropy paling sedikit dari entropy sampel. Atribut terbaik adalah atribut yang memiliki information gain tertinggi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2b. Penanganan label dari cabang setiap nilai atribut</h4>\n",
    "<h5>Algoritma ID3 pada buku Machine Learning Tom Mitchell vs library DecisionTreeClassifier</h5>\n",
    "Berbeda dengan Algoritma ID3 pada buku Machine Learning Tom Mitchell, label dari cabang setiap nilai atribut di DecisionTreeClassifier merupakan strategy yang digunakan untuk melakukan split pada setiap node, yang selain bergantung pada dataset, juga bergantung pada value dari splitter, yaitu ‘best’ berarti strategi terbaik yang digunakan, atau ‘random’ berarti strategi random terbaik yang digunakan.\n",
    "<h5>Algoritma ID3 pada buku Machine Learning Tom Mitchell vs library Id3Estimator</h5>\n",
    "Penanganan label dari cabang atribut di Algoritma ID3 pada buku Machine Learning Tom Mitchell sama dengan label dari cabang setiap nilai atribut di ID3Estimator, yaitu setiap value atau range dari value dari suatu atribut terbaik yang dapat mengklasifikasikan training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2c. Penentuan label jika examples kosong di cabang tersebut </h4>\n",
    "\n",
    "<h5>Algoritma ID3 pada buku Machine Learning Tom Mitchell vs library DecisionTreeClassifier</h5>\n",
    "Pada kedua algoritma, ketika examples kosong pada suatu cabang, di bawah cabang tersebut diberi daun dengan label berisi nilai yang paling umum dari target atribut di dalam examples.  Sebagai contoh, pada data pasien rumah sakit, ketika tidak terdapat pasien pria yang bergolongan darah AB dan secara umum pasien bergolongan darah AB memiliki label 2, maka pasien pria AB akan diberi label 2.\n",
    "\n",
    "<h5> Algoritma ID3 pada buku Machine Learning Tom Mitchell vs library Id3Estimator </h5>\n",
    "Pada kedua algoritma, ketika examples kosong pada suatu cabang, di bawah cabang tersebut diberi daun dengan label berisi nilai yang paling umum dari target atribut di dalam examples. Sebagai contoh, pada data pasien rumah sakit, ketika tidak terdapat pasien pria yang bergolongan darah AB dan secara umum pasien bergolongan darah AB memiliki label 2, maka pasien pria AB akan diberi label 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2d. Penanganan atribut kontinu </h4>\n",
    "<h5> Algoritma ID3 halaman 56 Machine Learning Tom Mitchell vs DecisionTreeClassifier </h5>\n",
    "Pada algoritma ID3 di halaman 56 Machine Learning Tom Mitchell, atribut kontinu dianggap diskrit sehingga untuk setiap nilai yang mungkin dari examples pada atribut tersebut akan diproses satu persatu.\n",
    " \tPada algoritma DecisionTreeClassifier, untuk atribut kontinu A, algoritma akan secara dinamik membentuk atribut boolean baru Ac yang bernilai true jika A < c dan bernilai false untuk sebaliknya. c dipilih dengan cara melakukan sorting terlebih dahulu examples berdasarkan atribut A kemudian dicari examples yang bertetangga yang klasifikasinya berbeda, kemudian dapat dihasilkan set kandidat batas c yang berada ditengah antara nilai-nilai atribut A tersebut. Dari set kandidat c tersebut dipilih c yang menghasilkan information gain terbaik.\n",
    "\n",
    "<h5> Algoritma ID3 halaman 56 Machine Learning Tom Mitchell vs Id3Estimator </h5> Pada algoritma ID3 di halaman 56 Machine Learning Tom Mitchell, atribut kontinu dianggap diskrit sehingga untuk setiap nilai yang mungkin dari examples pada atribut tersebut akan diproses satu persatu.\n",
    " \tPada algoritma Id3Estimator, untuk atribut kontinu A, algoritma akan secara dinamik membentuk atribut boolean baru Ac yang bernilai true jika A < c dan bernilai false untuk sebaliknya. c dipilih dengan cara melakukan sorting terlebih dahulu examples berdasarkan atribut A kemudian dicari examples yang bertetangga yang klasifikasinya berbeda, kemudian dapat dihasilkan set kandidat batas c yang berada ditengah antara nilai-nilai atribut A tersebut. Dari set kandidat c tersebut dipilih c yang menghasilkan information gain terbaik.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2e. Penanganan atribut dengan missing values</h4>\n",
    "<h5> Algoritma ID3 halaman 56 Machine Learning Tom Mitchell vs DecisionTreeClassifier </h5>\n",
    "Kedua algoritma mengabaikan atau tidak mendukung missing value (atau dianggap sebagai value baru jika missing value tersebut dikategorikan (e.g. diberi nilai sebagai nan, None). Sebagai contoh, pada data pasien rumah sakit, beberapa pasien bisa jadi tidak memiliki data golongan darah. Sehingga pada pohon keputusan “golongan darah = tidak ada” adalah sebuah cabang yang berbeda.\n",
    "\n",
    "<h5> Algoritma ID3 halaman 56 Machine Learning Tom MItchell vs Id3Estimator </h5>\n",
    "Kedua algoritma mengabaikan atau tidak mendukung missing value (atau dianggap sebagai value baru jika missing value tersebut dikategorikan (e.g. diberi nilai sebagai nan, None). Sebagai contoh, pada data pasien rumah sakit, beberapa pasien bisa jadi tidak memiliki data golongan darah. Sehingga pada pohon keputusan “golongan darah = tidak ada” adalah sebuah cabang yang berbeda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2f. Pruning dan parameter confidence</h4>\n",
    "<h5>Algoritma ID3 pada buku Machine Learning Tom Mitchell vs library DecisionTreeClassifier</h5>\n",
    "Berbeda dengan algortima ID3 pada buku Machine Learning Tom Mitchell yang tidak melakukan pruning sama sekali, DecisionTreeClassifier melakukan pruning pada decision tree sampai memenuhi parameter yang mengatur ukuran tree, seperti max_depth, min_samples_leaf, dan max_leaf_nodes. Parameter confidence untuk algoritma ini adalah error pada validasi atribut untuk semua item pada dataset, dan mengacu pada Minimal Cost-Complexity Pruning. Minimal Cost-Complexity Pruning menghilangkan subtree yang ketika dihilangkan menghasilkan minimum dari error tersebut.\n",
    "<h5>Algoritma ID3 pada buku Machine Learning Tom Mitchell vs library Id3Estimator</h5>\n",
    "Algoritma ID3 pada buku Machine Learning Tom Mitchell tidak melakukan pruning untuk mengoptimasi decision tree, dan berjalan sampai decision tree dapat mengklasifikasi training example secara sempurna. Algoritma ini juga tidak menggunakan parameter confidence, yaitu level yang digunakan untuk melakukan pruning.<br>\n",
    "Berbeda dengan algoritma ID3 pada buku Machine Learning Tom Mitchell yang tidak melakukan pruning sama sekali, Id3Estimator menggunakan parameter prune yang merupakan boolean yang jika diset sebagai true, maka akan melakukan pruning pada decision tree. Parameter confidence yang digunakan adalah error dari node yang akan diprune. Jika error dari node lebih kecil dibandingkan error dari childrennya, maka node tersebut akan diprune. Pruning dilakukan sampai memenuhi parameter yang mengatur ukuran tree, yaitu max_depth."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
